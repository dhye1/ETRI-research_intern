{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with BLEU Score"
      ],
      "metadata": {
        "id": "O2L1CcvFDeku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Metrics 관련 함수는 Oscar 논문 저자가 사용한 코드를 참고하여 구현 "
      ],
      "metadata": {
        "id": "41eWjTxhjXt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU"
      ],
      "metadata": {
        "id": "xuNrQJB1AcdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Provides:\n",
        "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
        "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
        "'''\n",
        "from builtins import range, dict\n",
        "\n",
        "import copy\n",
        "import sys, math, re\n",
        "from collections import defaultdict\n",
        "\n",
        "def precook(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    reflen = []\n",
        "    maxcounts = dict()\n",
        "    for ref in refs:\n",
        "        rl, counts = precook(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.items():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "\n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test(test, refs, eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    reflen, refmaxcounts = refs\n",
        "    testlen, counts = precook(test, n, True)\n",
        "\n",
        "    result = dict()\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "\n",
        "    result[\"testlen\"] = testlen\n",
        "\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
        "\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.items():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen\n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "\n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self\n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "\n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:\n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "\n",
        "            for key in ['guess','correct']:\n",
        "                for k in range(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in range(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small)\n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in range(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print(comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in range(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in range(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(totalcomps)\n",
        "            print(\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list"
      ],
      "metadata": {
        "id": "7LAMiW71ApIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lagxZXdB_4gu"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "#\n",
        "# File Name : bleu.py\n",
        "#\n",
        "# Description : Wrapper for BLEU scorer.\n",
        "#\n",
        "# Creation Date : 06-01-2015\n",
        "# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n",
        "# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
        "\n",
        "# from .bleu_scorer import BleuScorer\n",
        "\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        print(gts)\n",
        "        print(res)\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) >= 1)\n",
        "\n",
        "            bleu_scorer += (hypo[0], ref)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIBFBzJ29rJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "syx1ho75C_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "print(path_to_jar_dirname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKeMtBk-htrd",
        "outputId": "cc42f964-d429-4ef4-f343-c931f363e7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tempfile\n",
        "import itertools\n",
        "\n",
        "# path to the stanford corenlp jar\n",
        "STANFORD_CORENLP_3_4_1_JAR = 'stanford-corenlp-3.4.1.jar'\n",
        "\n",
        "# punctuations to be removed from the sentences\n",
        "PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\", \\\n",
        "        \".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"]\n",
        "\n",
        "class PTBTokenizer:\n",
        "    \"\"\"Python wrapper of Stanford PTBTokenizer\"\"\"\n",
        "\n",
        "    def tokenize(self, captions_for_image):\n",
        "        cmd = ['java', '-cp', STANFORD_CORENLP_3_4_1_JAR, \\\n",
        "                'edu.stanford.nlp.process.PTBTokenizer', \\\n",
        "                '-preserveLines', '-lowerCase']\n",
        "\n",
        "        # ======================================================\n",
        "        # prepare data for PTB Tokenizer\n",
        "        # ======================================================\n",
        "        final_tokenized_captions_for_image = {}\n",
        "        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n",
        "        sentences = '\\n'.join([c['caption'].replace('\\n', ' ') for k, v in captions_for_image.items() for c in v])\n",
        "        # print('imageid', len(image_id))\n",
        "        # print('sentences', len(sentences))\n",
        "\n",
        "        # ======================================================\n",
        "        # save sentences to temporary file\n",
        "        # ======================================================\n",
        "\n",
        "        # Get the absolute path of the current file's directory\n",
        "        path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "\n",
        "        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n",
        "        tmp_file.write(sentences.encode())\n",
        "        # print(tmp_file.tell() == 0)\n",
        "        tmp_file.close()\n",
        "        \n",
        "        # ======================================================\n",
        "        # tokenize sentence\n",
        "        # ======================================================\n",
        "        cmd.append(os.path.basename(tmp_file.name))\n",
        "        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n",
        "                stdout=subprocess.PIPE)\n",
        "        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n",
        "        token_lines = token_lines.decode()\n",
        "        lines = token_lines.split('\\n')\n",
        "        # print('lines', lines)\n",
        "        # remove temp file\n",
        "        os.remove(tmp_file.name)\n",
        "\n",
        "\n",
        "        # ======================================================\n",
        "        # create dictionary for tokenized captions\n",
        "        # ======================================================\n",
        "        for k, line in zip(image_id, lines):\n",
        "            # print(line)\n",
        "            if not k in final_tokenized_captions_for_image:\n",
        "                # print('not in final')\n",
        "                final_tokenized_captions_for_image[k] = []\n",
        "            tokenized_caption = ' '.join([w for w in line.rstrip().split(' ') \\\n",
        "                    if w not in PUNCTUATIONS])\n",
        "            # print('tokenized_caption', tokenized_caption)\n",
        "            final_tokenized_captions_for_image[k].append(tokenized_caption)\n",
        "\n",
        "\n",
        "        return final_tokenized_captions_for_image"
      ],
      "metadata": {
        "id": "ixRhvKT5CS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJFoPRgj9p2B",
        "outputId": "34df366c-5a1b-48b7-c1f6-534f898df2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval.py"
      ],
      "metadata": {
        "id": "KzL7PepKDcfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'tylin'\n",
        "from builtins import dict\n",
        "# from .tokenizer.ptbtokenizer import PTBTokenizer\n",
        "# from .bleu.bleu import Bleu\n",
        "# from .meteor.meteor import Meteor\n",
        "# from .rouge.rouge import Rouge\n",
        "# from .cider.cider import Cider\n",
        "# from .spice.spice import Spice\n",
        "\n",
        "class COCOEvalCap:\n",
        "    def __init__(self, coco, cocoRes):\n",
        "        self.evalImgs = []\n",
        "        self.eval = dict()\n",
        "        self.imgToEval = dict()\n",
        "        self.coco = coco\n",
        "        self.cocoRes = cocoRes\n",
        "        self.params = {'image_id': coco.getImgIds()}\n",
        "\n",
        "        self.gts = None\n",
        "        self.res = None\n",
        "        # self.df = df\n",
        "\n",
        "    def tokenize(self):\n",
        "        imgIds = self.params['image_id']\n",
        "        # imgIds = self.coco.getImgIds()\n",
        "        gts = dict()\n",
        "        res = dict()\n",
        "        for imgId in imgIds:\n",
        "            gts[imgId] = self.coco.imgToAnns[imgId]\n",
        "            res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenization...')\n",
        "        tokenizer = PTBTokenizer()\n",
        "        self.gts  = tokenizer.tokenize(gts)\n",
        "        self.res = tokenizer.tokenize(res)\n",
        "        # print(len(self.gts))\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.tokenize()\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"])\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(self.gts, self.res)\n",
        "            print(score)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, self.gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, self.gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = dict()\n",
        "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
      ],
      "metadata": {
        "id": "USQS0bQHDAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9999VzDDRW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EvalCapDemo"
      ],
      "metadata": {
        "id": "IjFfVOd-D82E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "from pycocotools.coco import COCO\n",
        "# from pycocoevalcap.eval import COCOEvalCap\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "import json\n",
        "from json import encoder\n",
        "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
      ],
      "metadata": {
        "id": "DlKX3I9RDRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create coco object and cocoRes object\n",
        "coco = COCO('/content/drive/MyDrive/논문/captions_val2014.json')\n",
        "cocoRes = coco.loadRes('/content/drive/MyDrive/논문/cocores.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1rHSD_UEALn",
        "outputId": "6a2ee7e4-c6e9-430b-f39d-a3c8343965d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.99s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create cocoEval object by taking coco and cocoRes\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "\n",
        "# evaluate on a subset of images by setting\n",
        "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "# please remove this line when evaluating the full validation set\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "\n",
        "# evaluate results\n",
        "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
        "cocoEval.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsUyo66JEKWh",
        "outputId": "a134b5d4-a3c6-4f83-ae32-35b36f073f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'testlen': 394647, 'reflen': 389817, 'guess': [394647, 354457, 314267, 274077], 'correct': [309680, 181692, 95661, 49839]}\n",
            "ratio: 1.0123904293553103\n",
            "[0.7847012646745046, 0.6342176152680618, 0.4965590625075106, 0.3862801276888551]\n",
            "Bleu_1: 0.785\n",
            "Bleu_2: 0.634\n",
            "Bleu_3: 0.497\n",
            "Bleu_4: 0.386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBJLcekrEsyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}