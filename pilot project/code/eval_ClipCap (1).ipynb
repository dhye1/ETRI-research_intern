{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## COCO로 훈련된 CLIPCAP 학습 및 평가"
      ],
      "metadata": {
        "id": "O2L1CcvFDeku"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfpGaz27IWs",
        "outputId": "435cad7f-96e9-4e1b-c7d9-3090c3e32fe2"
      },
      "source": [
        "#@title Install\n",
        "!pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-r79wwyt7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-r79wwyt7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=a122b91e10acf5709e6326999b9dcc49f385356ef4907a1960de5b30f0248210\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9v3yopz/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqE3Fj5-uYSR"
      },
      "source": [
        "#@title Drive Downloader\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}  \n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OArDkm_24w4L"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image \n",
        "\n",
        "\n",
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model_path = os.path.join(save_path, 'model_wieghts.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ClW2ebek8DK"
      },
      "source": [
        "#@title Model\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7xocT3TUgey"
      },
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE-uUStuv1Nl"
      },
      "source": [
        "#@title Choose pretrained model - COCO or Coneptual captions\n",
        "\n",
        "\n",
        "pretrained_model = 'COCO'  # @param ['COCO', 'Conceptual captions']\n",
        "\n",
        "if pretrained_model == 'Conceptual captions':\n",
        "  downloader.download_file(\"14pXWwB4Zm82rsDdvbGguLfx9F8aM7ovT\", model_path)\n",
        "else:\n",
        "  downloader.download_file(\"1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX\", model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCgFHSgr_ny"
      },
      "source": [
        "#@title GPU/CPU\n",
        "\n",
        "\n",
        "is_gpu = True #@param {type:\"boolean\"}  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bi_2zQ3QD57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "005077a837444c00b68ef75cbd29d408",
            "35bc94599a6041d8ad2097a6cc1767c7",
            "0d435b04ec8749f28ef80957f1565049",
            "45a160a4fa2042aea67abff2fafb353f",
            "177c4112ebb9429d91d798dc3c6ac231",
            "a13a24647453464dbc071bb2c52683e4",
            "125b6b0526174506bf1bb33d7a7223c5",
            "3d6b11f6e2004a3c8809a2119d270221",
            "4ed4592145a24b09ad3f5f9a77a2a331",
            "04355953f67e401ea4ac4835f740291e",
            "e772c72cc29e43699f0d39c0ec4de711",
            "63403943f438407d9eb271fbe9b059e5",
            "fec7a1d0e59b4efd9049b3ac44f07b08",
            "dfb7cc078b70455e9f823f1a2286f91e",
            "a7cdff00e209493caa95c99dd22eb68e",
            "6b6d1a35c0a245909ba10a786a55f5ba",
            "1026eb65db484b5e84d0948fe7385443",
            "c670398a22714b7c8d1732c11a0abd25",
            "74c0d817a2ba44d3bc5c0108685353b1",
            "c7a2cd10a23b4d40ae4b7e56934a8ef2",
            "01add01be3cf4b0692e1a67e16ba631e",
            "7505f52c9bfe4faa9141207b3aa7787f",
            "f18416db2b774068bbc191567f7427e5",
            "c7b70b3994c8411c934471ab79f34b38",
            "e160594d61ca48628992266e7c2bb9c6",
            "9bfec0cf05b54534aa9e0952a1631cb5",
            "1f4dfe1f134c4a498856759c25fa9ec8",
            "89774552fc2e40b98448edd8ac81d974",
            "ecc5e1b04e264ee291174a574e886b38",
            "5dd8dd74c65f4af986033e35ee12f263",
            "8c866fd7daab41b8b0a3a2e32c6ffc2b",
            "3bc82f851f1f47f3b08e8fc66842210f",
            "0168c7d582bc457786b43a72f99af8f9"
          ]
        },
        "outputId": "d590d2aa-99f0-4649-b476-a1318260c774"
      },
      "source": [
        "#@title CLIP model + GPT2 tokenizer\n",
        "\n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 150MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "005077a837444c00b68ef75cbd29d408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63403943f438407d9eb271fbe9b059e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f18416db2b774068bbc191567f7427e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a3a328fb12fd4a149218220540f5e23d",
            "a99d2aa26a264e858d3143e7a5bb8a91",
            "36a19c5c8aa245aa8f1ad7c339cad30f",
            "82ce5b584eaa491ba0832938a212937b",
            "a52cef4f7f0341c2b99b6b5115b3398f",
            "81c89ccd2154498e983444a55b4c48d4",
            "c2d770b3d7a74241bc24ffcd82763af8",
            "956e3ab405834dc0b6c01567a92d2f07",
            "06bb1563ab35461a85bd8f0a647d1856",
            "57395215f2254440a5f1626cbb54bcf2",
            "f90049dcf9ce4cb2b0b51c05109f4a2e",
            "a2dc396e5d534a078b9998ab90b1175b",
            "8108b8e741d04f66a86f3451b0b04759",
            "9a2f8d149be0414480cf2bf28d6431f8",
            "f5e221e2f1544edfb4afc53f08d46218",
            "75d69b2113c04fe9be5f96987ea542a2",
            "1e67483df4a24330983fd1004dd164a4",
            "7123455678fb4d2b8df3f18fd827351e",
            "361c3ab8028747828cd699fb3ee8c10e",
            "438c1b58537743638428c9fdb93551fd",
            "ee7975c0aaff4e22aabd9e9edfb32feb",
            "0451a48535b945c28ff326758a8afde2"
          ]
        },
        "id": "glBzYsgIwhwF",
        "outputId": "eb349c52-ab7d-4fe4-ea6c-437a3d4d2e91"
      },
      "source": [
        "#@title Load model weights\n",
        "\n",
        "\n",
        "prefix_length = 10\n",
        "\n",
        "model = ClipCaptionModel(prefix_length)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
        "\n",
        "model = model.eval() \n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3a328fb12fd4a149218220540f5e23d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2dc396e5d534a078b9998ab90b1175b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: coco_train clipcap model"
      ],
      "metadata": {
        "id": "WAFwjtrJDAgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bleu"
      ],
      "metadata": {
        "id": "xuNrQJB1AcdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Provides:\n",
        "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
        "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
        "'''\n",
        "from builtins import range, dict\n",
        "\n",
        "import copy\n",
        "import sys, math, re\n",
        "from collections import defaultdict\n",
        "\n",
        "def precook(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    reflen = []\n",
        "    maxcounts = dict()\n",
        "    for ref in refs:\n",
        "        rl, counts = precook(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.items():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "\n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test(test, refs, eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    reflen, refmaxcounts = refs\n",
        "    testlen, counts = precook(test, n, True)\n",
        "\n",
        "    result = dict()\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "\n",
        "    result[\"testlen\"] = testlen\n",
        "\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
        "\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.items():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen\n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "\n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self\n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "\n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:\n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "\n",
        "            for key in ['guess','correct']:\n",
        "                for k in range(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in range(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small)\n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in range(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print(comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in range(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in range(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(totalcomps)\n",
        "            print(\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list"
      ],
      "metadata": {
        "id": "7LAMiW71ApIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lagxZXdB_4gu"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "#\n",
        "# File Name : bleu.py\n",
        "#\n",
        "# Description : Wrapper for BLEU scorer.\n",
        "#\n",
        "# Creation Date : 06-01-2015\n",
        "# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n",
        "# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
        "\n",
        "# from .bleu_scorer import BleuScorer\n",
        "\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        print(gts)\n",
        "        print(res)\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) >= 1)\n",
        "\n",
        "            bleu_scorer += (hypo[0], ref)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIBFBzJ29rJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "syx1ho75C_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "print(path_to_jar_dirname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKeMtBk-htrd",
        "outputId": "cc42f964-d429-4ef4-f343-c931f363e7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tempfile\n",
        "import itertools\n",
        "\n",
        "# path to the stanford corenlp jar\n",
        "STANFORD_CORENLP_3_4_1_JAR = 'stanford-corenlp-3.4.1.jar'\n",
        "\n",
        "# punctuations to be removed from the sentences\n",
        "PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\", \\\n",
        "        \".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"]\n",
        "\n",
        "class PTBTokenizer:\n",
        "    \"\"\"Python wrapper of Stanford PTBTokenizer\"\"\"\n",
        "\n",
        "    def tokenize(self, captions_for_image):\n",
        "        cmd = ['java', '-cp', STANFORD_CORENLP_3_4_1_JAR, \\\n",
        "                'edu.stanford.nlp.process.PTBTokenizer', \\\n",
        "                '-preserveLines', '-lowerCase']\n",
        "\n",
        "        # ======================================================\n",
        "        # prepare data for PTB Tokenizer\n",
        "        # ======================================================\n",
        "        final_tokenized_captions_for_image = {}\n",
        "        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n",
        "        sentences = '\\n'.join([c['caption'].replace('\\n', ' ') for k, v in captions_for_image.items() for c in v])\n",
        "        # print('imageid', len(image_id))\n",
        "        # print('sentences', len(sentences))\n",
        "\n",
        "        # ======================================================\n",
        "        # save sentences to temporary file\n",
        "        # ======================================================\n",
        "\n",
        "        # Get the absolute path of the current file's directory\n",
        "        path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "\n",
        "        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n",
        "        tmp_file.write(sentences.encode())\n",
        "        # print(tmp_file.tell() == 0)\n",
        "        tmp_file.close()\n",
        "        \n",
        "        # ======================================================\n",
        "        # tokenize sentence\n",
        "        # ======================================================\n",
        "        cmd.append(os.path.basename(tmp_file.name))\n",
        "        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n",
        "                stdout=subprocess.PIPE)\n",
        "        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n",
        "        token_lines = token_lines.decode()\n",
        "        lines = token_lines.split('\\n')\n",
        "        # print('lines', lines)\n",
        "        # remove temp file\n",
        "        os.remove(tmp_file.name)\n",
        "\n",
        "\n",
        "        # ======================================================\n",
        "        # create dictionary for tokenized captions\n",
        "        # ======================================================\n",
        "        for k, line in zip(image_id, lines):\n",
        "            # print(line)\n",
        "            if not k in final_tokenized_captions_for_image:\n",
        "                # print('not in final')\n",
        "                final_tokenized_captions_for_image[k] = []\n",
        "            tokenized_caption = ' '.join([w for w in line.rstrip().split(' ') \\\n",
        "                    if w not in PUNCTUATIONS])\n",
        "            # print('tokenized_caption', tokenized_caption)\n",
        "            final_tokenized_captions_for_image[k].append(tokenized_caption)\n",
        "\n",
        "\n",
        "        return final_tokenized_captions_for_image"
      ],
      "metadata": {
        "id": "ixRhvKT5CS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJFoPRgj9p2B",
        "outputId": "34df366c-5a1b-48b7-c1f6-534f898df2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coco.py"
      ],
      "metadata": {
        "id": "mwVNYVgpDcjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "__author__ = 'tylin'\n",
        "__version__ = '1.0.1'\n",
        "# Interface for accessing the Microsoft COCO dataset.\n",
        "\n",
        "# Microsoft COCO is a large image dataset designed for object detection,\n",
        "# segmentation, and caption generation. pycocotools is a Python API that\n",
        "# assists in loading, parsing and visualizing the annotations in COCO.\n",
        "# Please visit http://mscoco.org/ for more information on COCO, including\n",
        "# for the data, paper, and tutorials. The exact format of the annotations\n",
        "# is also described on the COCO website. For example usage of the pycocotools\n",
        "# please see pycocotools_demo.ipynb. In addition to this API, please download both\n",
        "# the COCO images and annotations in order to run the demo.\n",
        "\n",
        "# An alternative to using the API is to load the annotations directly\n",
        "# into Python dictionary\n",
        "# Using the API provides additional utility functions. Note that this API\n",
        "# supports both *instance* and *caption* annotations. In the case of\n",
        "# captions not all functions are defined (e.g. categories are undefined).\n",
        "\n",
        "# The following API functions are defined:\n",
        "#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n",
        "#  decodeMask - Decode binary mask M encoded via run-length encoding.\n",
        "#  encodeMask - Encode binary mask M using run-length encoding.\n",
        "#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n",
        "#  getCatIds  - Get cat ids that satisfy given filter conditions.\n",
        "#  getImgIds  - Get img ids that satisfy given filter conditions.\n",
        "#  loadAnns   - Load anns with the specified ids.\n",
        "#  loadCats   - Load cats with the specified ids.\n",
        "#  loadImgs   - Load imgs with the specified ids.\n",
        "#  segToMask  - Convert polygon segmentation to binary mask.\n",
        "#  showAnns   - Display the specified annotations.\n",
        "#  loadRes    - Load result file and create result api object.\n",
        "# Throughout the API \"ann\"=annotation, \"cat\"=category, and \"img\"=image.\n",
        "# Help on each functions can be accessed by: \"help COCO>function\".\n",
        "\n",
        "# See also COCO>decodeMask,\n",
        "# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n",
        "# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n",
        "# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n",
        "\n",
        "# Microsoft COCO Toolbox.      Version 1.0\n",
        "# Data, paper, and tutorials available at:  http://mscoco.org/\n",
        "# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n",
        "# Licensed under the Simplified BSD License [see bsd.txt]\n",
        "from builtins import int\n",
        "\n",
        "import json\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Polygon\n",
        "import numpy as np\n",
        "from skimage.draw import polygon\n",
        "import copy\n",
        "\n",
        "class COCO:\n",
        "    def __init__(self, annotation_file=None):\n",
        "        \"\"\"\n",
        "        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n",
        "        :param annotation_file (str): location of annotation file\n",
        "        :param image_folder (str): location to the folder that hosts images.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # load dataset\n",
        "        self.dataset = {}\n",
        "        self.anns = []\n",
        "        self.imgToAnns = {}\n",
        "        self.catToImgs = {}\n",
        "        self.imgs = []\n",
        "        self.cats = []\n",
        "        if not annotation_file == None:\n",
        "            print('loading annotations into memory...')\n",
        "            time_t = datetime.datetime.utcnow()\n",
        "            dataset = json.load(open(annotation_file, 'r'))\n",
        "            print(datetime.datetime.utcnow() - time_t)\n",
        "            self.dataset = dataset\n",
        "            self.createIndex()\n",
        "\n",
        "    def createIndex(self):\n",
        "        # create index\n",
        "        print('creating index...')\n",
        "        imgToAnns = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
        "        anns =      {ann['id']:       [] for ann in self.dataset['annotations']}\n",
        "        for ann in self.dataset['annotations']:\n",
        "            imgToAnns[ann['image_id']] += [ann]\n",
        "            anns[ann['id']] = ann\n",
        "\n",
        "        imgs      = {im['id']: {} for im in self.dataset['images']}\n",
        "        for img in self.dataset['images']:\n",
        "            imgs[img['id']] = img\n",
        "\n",
        "        cats = []\n",
        "        catToImgs = []\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            cats = {cat['id']: [] for cat in self.dataset['categories']}\n",
        "            for cat in self.dataset['categories']:\n",
        "                cats[cat['id']] = cat\n",
        "            catToImgs = {cat['id']: [] for cat in self.dataset['categories']}\n",
        "            for ann in self.dataset['annotations']:\n",
        "                catToImgs[ann['category_id']] += [ann['image_id']]\n",
        "\n",
        "        print('index created!')\n",
        "\n",
        "        # create class members\n",
        "        self.anns = anns\n",
        "        self.imgToAnns = imgToAnns\n",
        "        self.catToImgs = catToImgs\n",
        "        self.imgs = imgs\n",
        "        self.cats = cats\n",
        "\n",
        "    def info(self):\n",
        "        \"\"\"\n",
        "        Print information about the annotation file.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for key, value in self.datset['info'].items():\n",
        "            print('%s: %s'%(key, value))\n",
        "\n",
        "    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n",
        "        \"\"\"\n",
        "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
        "        :param imgIds  (int array)     : get anns for given imgs\n",
        "               catIds  (int array)     : get anns for given cats\n",
        "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
        "               iscrowd (boolean)       : get anns for given crowd label (False or True)\n",
        "        :return: ids (int array)       : integer array of ann ids\n",
        "        \"\"\"\n",
        "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n",
        "            anns = self.dataset['annotations']\n",
        "        else:\n",
        "            if not len(imgIds) == 0:\n",
        "                anns = sum([self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns],[])\n",
        "            else:\n",
        "                anns = self.dataset['annotations']\n",
        "            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n",
        "            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            if not iscrowd == None:\n",
        "                ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n",
        "            else:\n",
        "                ids = [ann['id'] for ann in anns]\n",
        "        else:\n",
        "            ids = [ann['id'] for ann in anns]\n",
        "        return ids\n",
        "\n",
        "    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n",
        "        \"\"\"\n",
        "        filtering parameters. default skips that filter.\n",
        "        :param catNms (str array)  : get cats for given cat names\n",
        "        :param supNms (str array)  : get cats for given supercategory names\n",
        "        :param catIds (int array)  : get cats for given cat ids\n",
        "        :return: ids (int array)   : integer array of cat ids\n",
        "        \"\"\"\n",
        "        catNms = catNms if type(catNms) == list else [catNms]\n",
        "        supNms = supNms if type(supNms) == list else [supNms]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(catNms) == len(supNms) == len(catIds) == 0:\n",
        "            cats = self.dataset['categories']\n",
        "        else:\n",
        "            cats = self.dataset['categories']\n",
        "            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n",
        "            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n",
        "            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n",
        "        ids = [cat['id'] for cat in cats]\n",
        "        return ids\n",
        "\n",
        "    def getImgIds(self, imgIds=[], catIds=[]):\n",
        "        '''\n",
        "        Get img ids that satisfy given filter conditions.\n",
        "        :param imgIds (int array) : get imgs for given ids\n",
        "        :param catIds (int array) : get imgs with all given cats\n",
        "        :return: ids (int array)  : integer array of img ids\n",
        "        '''\n",
        "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            ids = self.imgs.keys()\n",
        "        else:\n",
        "            ids = set(imgIds)\n",
        "            for catId in catIds:\n",
        "                if len(ids) == 0:\n",
        "                    ids = set(self.catToImgs[catId])\n",
        "                else:\n",
        "                    ids &= set(self.catToImgs[catId])\n",
        "        return list(ids)\n",
        "\n",
        "    def loadAnns(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying anns\n",
        "        :return: anns (object array) : loaded ann objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.anns[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.anns[ids]]\n",
        "\n",
        "    def loadCats(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load cats with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying cats\n",
        "        :return: cats (object array) : loaded cat objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.cats[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.cats[ids]]\n",
        "\n",
        "    def loadImgs(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying img\n",
        "        :return: imgs (object array) : loaded img objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.imgs[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.imgs[ids]]\n",
        "\n",
        "    def showAnns(self, anns):\n",
        "        \"\"\"\n",
        "        Display the specified annotations.\n",
        "        :param anns (array of object): annotations to display\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        if len(anns) == 0:\n",
        "            return 0\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            ax = plt.gca()\n",
        "            polygons = []\n",
        "            color = []\n",
        "            for ann in anns:\n",
        "                c = np.random.random((1, 3)).tolist()[0]\n",
        "                if type(ann['segmentation']) == list:\n",
        "                    # polygon\n",
        "                    for seg in ann['segmentation']:\n",
        "                        poly = np.array(seg).reshape((len(seg)//2, 2))\n",
        "                        polygons.append(Polygon(poly, True,alpha=0.4))\n",
        "                        color.append(c)\n",
        "                else:\n",
        "                    # mask\n",
        "                    mask = COCO.decodeMask(ann['segmentation'])\n",
        "                    img = np.ones( (mask.shape[0], mask.shape[1], 3) )\n",
        "                    if ann['iscrowd'] == 1:\n",
        "                        color_mask = np.array([2.0,166.0,101.0])/255\n",
        "                    if ann['iscrowd'] == 0:\n",
        "                        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "                    for i in range(3):\n",
        "                        img[:,:,i] = color_mask[i]\n",
        "                    ax.imshow(np.dstack( (img, mask*0.5) ))\n",
        "            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n",
        "            ax.add_collection(p)\n",
        "        if self.dataset['type'] == 'captions':\n",
        "            for ann in anns:\n",
        "                print(ann['caption'])\n",
        "\n",
        "    def loadRes(self, resFile):\n",
        "        \"\"\"\n",
        "        Load result file and return a result api object.\n",
        "        :param   resFile (str)     : file name of result file\n",
        "        :return: res (obj)         : result api object\n",
        "        \"\"\"\n",
        "        res = COCO()\n",
        "        res.dataset['images'] = [img for img in self.dataset['images']]\n",
        "        res.dataset['info'] = copy.deepcopy(self.dataset['info'])\n",
        "        res.dataset['type'] = copy.deepcopy(self.dataset['type'])\n",
        "        res.dataset['licenses'] = copy.deepcopy(self.dataset['licenses'])\n",
        "\n",
        "        print('Loading and preparing results...     ')\n",
        "        time_t = datetime.datetime.utcnow()\n",
        "        anns    = json.load(open(resFile))\n",
        "        assert type(anns) == list, 'results in not an array of objects'\n",
        "        annsImgIds = [ann['image_id'] for ann in anns]\n",
        "        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
        "               'Results do not correspond to current coco set'\n",
        "        if 'caption' in anns[0]:\n",
        "            imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n",
        "            res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n",
        "            for id_, ann in enumerate(anns):\n",
        "                ann['id'] = id_\n",
        "        elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n",
        "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "            for id_, ann in enumerate(anns):\n",
        "                bb = ann['bbox']\n",
        "                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n",
        "                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
        "                ann['area'] = bb[2]*bb[3]\n",
        "                ann['id'] = id_\n",
        "                ann['iscrowd'] = 0\n",
        "        elif 'segmentation' in anns[0]:\n",
        "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "            for id_, ann in enumerate(anns):\n",
        "                ann['area']=sum(ann['segmentation']['counts'][2:-1:2])\n",
        "                ann['bbox'] = []\n",
        "                ann['id'] = id_\n",
        "                ann['iscrowd'] = 0\n",
        "        print('DONE (t=%0.2fs)'%((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
        "\n",
        "        res.dataset['annotations'] = anns\n",
        "        res.createIndex()\n",
        "        return res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def decodeMask(R):\n",
        "        \"\"\"\n",
        "        Decode binary mask M encoded via run-length encoding.\n",
        "        :param   R (object RLE)    : run-length encoding of binary mask\n",
        "        :return: M (bool 2D array) : decoded binary mask\n",
        "        \"\"\"\n",
        "        N = len(R['counts'])\n",
        "        M = np.zeros( (R['size'][0]*R['size'][1], ))\n",
        "        n = 0\n",
        "        val = 1\n",
        "        for pos in range(N):\n",
        "            val = not val\n",
        "            for c in range(R['counts'][pos]):\n",
        "                R['counts'][pos]\n",
        "                M[n] = val\n",
        "                n += 1\n",
        "        return M.reshape((R['size']), order='F')\n",
        "\n",
        "    @staticmethod\n",
        "    def encodeMask(M):\n",
        "        \"\"\"\n",
        "        Encode binary mask M using run-length encoding.\n",
        "        :param   M (bool 2D array)  : binary mask to encode\n",
        "        :return: R (object RLE)     : run-length encoding of binary mask\n",
        "        \"\"\"\n",
        "        [h, w] = M.shape\n",
        "        M = M.flatten(order='F')\n",
        "        N = len(M)\n",
        "        counts_list = []\n",
        "        pos = 0\n",
        "        # counts\n",
        "        counts_list.append(1)\n",
        "        diffs = np.logical_xor(M[0:N-1], M[1:N])\n",
        "        for diff in diffs:\n",
        "            if diff:\n",
        "                pos +=1\n",
        "                counts_list.append(1)\n",
        "            else:\n",
        "                counts_list[pos] += 1\n",
        "        # if array starts from 1. start with 0 counts for 0\n",
        "        if M[0] == 1:\n",
        "            counts_list = [0] + counts_list\n",
        "        return {'size':      [h, w],\n",
        "               'counts':    counts_list ,\n",
        "               }\n",
        "\n",
        "    @staticmethod\n",
        "    def segToMask( S, h, w ):\n",
        "         \"\"\"\n",
        "         Convert polygon segmentation to binary mask.\n",
        "         :param   S (float array)   : polygon segmentation mask\n",
        "         :param   h (int)           : target mask height\n",
        "         :param   w (int)           : target mask width\n",
        "         :return: M (bool 2D array) : binary mask\n",
        "         \"\"\"\n",
        "         M = np.zeros((h,w), dtype=np.bool)\n",
        "         for s in S:\n",
        "             N = len(s)\n",
        "             rr, cc = polygon(np.array(s[1:N:2]), np.array(s[0:N:2])) # (y, x)\n",
        "             M[rr, cc] = 1\n",
        "         return M"
      ],
      "metadata": {
        "id": "WC8YSfVBDinF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval.py"
      ],
      "metadata": {
        "id": "KzL7PepKDcfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'tylin'\n",
        "from builtins import dict\n",
        "# from .tokenizer.ptbtokenizer import PTBTokenizer\n",
        "# from .bleu.bleu import Bleu\n",
        "# from .meteor.meteor import Meteor\n",
        "# from .rouge.rouge import Rouge\n",
        "# from .cider.cider import Cider\n",
        "# from .spice.spice import Spice\n",
        "\n",
        "class COCOEvalCap:\n",
        "    def __init__(self, coco, cocoRes):\n",
        "        self.evalImgs = []\n",
        "        self.eval = dict()\n",
        "        self.imgToEval = dict()\n",
        "        self.coco = coco\n",
        "        self.cocoRes = cocoRes\n",
        "        self.params = {'image_id': coco.getImgIds()}\n",
        "\n",
        "        self.gts = None\n",
        "        self.res = None\n",
        "        # self.df = df\n",
        "\n",
        "    def tokenize(self):\n",
        "        imgIds = self.params['image_id']\n",
        "        # imgIds = self.coco.getImgIds()\n",
        "        gts = dict()\n",
        "        res = dict()\n",
        "        for imgId in imgIds:\n",
        "            gts[imgId] = self.coco.imgToAnns[imgId]\n",
        "            res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenization...')\n",
        "        tokenizer = PTBTokenizer()\n",
        "        self.gts  = tokenizer.tokenize(gts)\n",
        "        self.res = tokenizer.tokenize(res)\n",
        "        # print(len(self.gts))\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.tokenize()\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"])\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(self.gts, self.res)\n",
        "            print(score)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, self.gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, self.gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = dict()\n",
        "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
      ],
      "metadata": {
        "id": "USQS0bQHDAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9999VzDDRW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EvalCapDemo"
      ],
      "metadata": {
        "id": "IjFfVOd-D82E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "from pycocotools.coco import COCO\n",
        "# from pycocoevalcap.eval import COCOEvalCap\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "import json\n",
        "from json import encoder\n",
        "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
      ],
      "metadata": {
        "id": "DlKX3I9RDRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create coco object and cocoRes object\n",
        "coco = COCO('/content/drive/MyDrive/논문/captions_val2014.json')\n",
        "cocoRes = coco.loadRes('/content/drive/MyDrive/논문/cocores.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1rHSD_UEALn",
        "outputId": "6a2ee7e4-c6e9-430b-f39d-a3c8343965d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.99s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create cocoEval object by taking coco and cocoRes\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "\n",
        "# evaluate on a subset of images by setting\n",
        "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "# please remove this line when evaluating the full validation set\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "\n",
        "# evaluate results\n",
        "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
        "cocoEval.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsUyo66JEKWh",
        "outputId": "a134b5d4-a3c6-4f83-ae32-35b36f073f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'testlen': 394647, 'reflen': 389817, 'guess': [394647, 354457, 314267, 274077], 'correct': [309680, 181692, 95661, 49839]}\n",
            "ratio: 1.0123904293553103\n",
            "[0.7847012646745046, 0.6342176152680618, 0.4965590625075106, 0.3862801276888551]\n",
            "Bleu_1: 0.785\n",
            "Bleu_2: 0.634\n",
            "Bleu_3: 0.497\n",
            "Bleu_4: 0.386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBJLcekrEsyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MsKG9JGCDcIa"
      }
    }
  ]
}