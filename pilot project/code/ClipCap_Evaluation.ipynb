{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with BLEU Score"
      ],
      "metadata": {
        "id": "O2L1CcvFDeku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU"
      ],
      "metadata": {
        "id": "xuNrQJB1AcdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Provides:\n",
        "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
        "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
        "'''\n",
        "from builtins import range, dict\n",
        "\n",
        "import copy\n",
        "import sys, math, re\n",
        "from collections import defaultdict\n",
        "\n",
        "def precook(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    reflen = []\n",
        "    maxcounts = dict()\n",
        "    for ref in refs:\n",
        "        rl, counts = precook(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.items():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "\n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test(test, refs, eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    reflen, refmaxcounts = refs\n",
        "    testlen, counts = precook(test, n, True)\n",
        "\n",
        "    result = dict()\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "\n",
        "    result[\"testlen\"] = testlen\n",
        "\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
        "\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.items():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen\n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "\n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self\n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "\n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:\n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "\n",
        "            for key in ['guess','correct']:\n",
        "                for k in range(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in range(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small)\n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in range(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print(comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in range(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in range(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(totalcomps)\n",
        "            print(\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list"
      ],
      "metadata": {
        "id": "7LAMiW71ApIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lagxZXdB_4gu"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "#\n",
        "# File Name : bleu.py\n",
        "#\n",
        "# Description : Wrapper for BLEU scorer.\n",
        "#\n",
        "# Creation Date : 06-01-2015\n",
        "# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n",
        "# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
        "\n",
        "# from .bleu_scorer import BleuScorer\n",
        "\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        print(gts)\n",
        "        print(res)\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) >= 1)\n",
        "\n",
        "            bleu_scorer += (hypo[0], ref)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIBFBzJ29rJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "syx1ho75C_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "print(path_to_jar_dirname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKeMtBk-htrd",
        "outputId": "cc42f964-d429-4ef4-f343-c931f363e7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tempfile\n",
        "import itertools\n",
        "\n",
        "# path to the stanford corenlp jar\n",
        "STANFORD_CORENLP_3_4_1_JAR = 'stanford-corenlp-3.4.1.jar'\n",
        "\n",
        "# punctuations to be removed from the sentences\n",
        "PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\", \\\n",
        "        \".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"]\n",
        "\n",
        "class PTBTokenizer:\n",
        "    \"\"\"Python wrapper of Stanford PTBTokenizer\"\"\"\n",
        "\n",
        "    def tokenize(self, captions_for_image):\n",
        "        cmd = ['java', '-cp', STANFORD_CORENLP_3_4_1_JAR, \\\n",
        "                'edu.stanford.nlp.process.PTBTokenizer', \\\n",
        "                '-preserveLines', '-lowerCase']\n",
        "\n",
        "        # ======================================================\n",
        "        # prepare data for PTB Tokenizer\n",
        "        # ======================================================\n",
        "        final_tokenized_captions_for_image = {}\n",
        "        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n",
        "        sentences = '\\n'.join([c['caption'].replace('\\n', ' ') for k, v in captions_for_image.items() for c in v])\n",
        "        # print('imageid', len(image_id))\n",
        "        # print('sentences', len(sentences))\n",
        "\n",
        "        # ======================================================\n",
        "        # save sentences to temporary file\n",
        "        # ======================================================\n",
        "\n",
        "        # Get the absolute path of the current file's directory\n",
        "        path_to_jar_dirname = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "\n",
        "        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n",
        "        tmp_file.write(sentences.encode())\n",
        "        # print(tmp_file.tell() == 0)\n",
        "        tmp_file.close()\n",
        "        \n",
        "        # ======================================================\n",
        "        # tokenize sentence\n",
        "        # ======================================================\n",
        "        cmd.append(os.path.basename(tmp_file.name))\n",
        "        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n",
        "                stdout=subprocess.PIPE)\n",
        "        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n",
        "        token_lines = token_lines.decode()\n",
        "        lines = token_lines.split('\\n')\n",
        "        # print('lines', lines)\n",
        "        # remove temp file\n",
        "        os.remove(tmp_file.name)\n",
        "\n",
        "\n",
        "        # ======================================================\n",
        "        # create dictionary for tokenized captions\n",
        "        # ======================================================\n",
        "        for k, line in zip(image_id, lines):\n",
        "            # print(line)\n",
        "            if not k in final_tokenized_captions_for_image:\n",
        "                # print('not in final')\n",
        "                final_tokenized_captions_for_image[k] = []\n",
        "            tokenized_caption = ' '.join([w for w in line.rstrip().split(' ') \\\n",
        "                    if w not in PUNCTUATIONS])\n",
        "            # print('tokenized_caption', tokenized_caption)\n",
        "            final_tokenized_captions_for_image[k].append(tokenized_caption)\n",
        "\n",
        "\n",
        "        return final_tokenized_captions_for_image"
      ],
      "metadata": {
        "id": "ixRhvKT5CS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJFoPRgj9p2B",
        "outputId": "34df366c-5a1b-48b7-c1f6-534f898df2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coco.py"
      ],
      "metadata": {
        "id": "mwVNYVgpDcjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "__author__ = 'tylin'\n",
        "__version__ = '1.0.1'\n",
        "# Interface for accessing the Microsoft COCO dataset.\n",
        "\n",
        "# Microsoft COCO is a large image dataset designed for object detection,\n",
        "# segmentation, and caption generation. pycocotools is a Python API that\n",
        "# assists in loading, parsing and visualizing the annotations in COCO.\n",
        "# Please visit http://mscoco.org/ for more information on COCO, including\n",
        "# for the data, paper, and tutorials. The exact format of the annotations\n",
        "# is also described on the COCO website. For example usage of the pycocotools\n",
        "# please see pycocotools_demo.ipynb. In addition to this API, please download both\n",
        "# the COCO images and annotations in order to run the demo.\n",
        "\n",
        "# An alternative to using the API is to load the annotations directly\n",
        "# into Python dictionary\n",
        "# Using the API provides additional utility functions. Note that this API\n",
        "# supports both *instance* and *caption* annotations. In the case of\n",
        "# captions not all functions are defined (e.g. categories are undefined).\n",
        "\n",
        "# The following API functions are defined:\n",
        "#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n",
        "#  decodeMask - Decode binary mask M encoded via run-length encoding.\n",
        "#  encodeMask - Encode binary mask M using run-length encoding.\n",
        "#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n",
        "#  getCatIds  - Get cat ids that satisfy given filter conditions.\n",
        "#  getImgIds  - Get img ids that satisfy given filter conditions.\n",
        "#  loadAnns   - Load anns with the specified ids.\n",
        "#  loadCats   - Load cats with the specified ids.\n",
        "#  loadImgs   - Load imgs with the specified ids.\n",
        "#  segToMask  - Convert polygon segmentation to binary mask.\n",
        "#  showAnns   - Display the specified annotations.\n",
        "#  loadRes    - Load result file and create result api object.\n",
        "# Throughout the API \"ann\"=annotation, \"cat\"=category, and \"img\"=image.\n",
        "# Help on each functions can be accessed by: \"help COCO>function\".\n",
        "\n",
        "# See also COCO>decodeMask,\n",
        "# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n",
        "# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n",
        "# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n",
        "\n",
        "# Microsoft COCO Toolbox.      Version 1.0\n",
        "# Data, paper, and tutorials available at:  http://mscoco.org/\n",
        "# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n",
        "# Licensed under the Simplified BSD License [see bsd.txt]\n",
        "from builtins import int\n",
        "\n",
        "import json\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Polygon\n",
        "import numpy as np\n",
        "from skimage.draw import polygon\n",
        "import copy\n",
        "\n",
        "class COCO:\n",
        "    def __init__(self, annotation_file=None):\n",
        "        \"\"\"\n",
        "        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n",
        "        :param annotation_file (str): location of annotation file\n",
        "        :param image_folder (str): location to the folder that hosts images.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # load dataset\n",
        "        self.dataset = {}\n",
        "        self.anns = []\n",
        "        self.imgToAnns = {}\n",
        "        self.catToImgs = {}\n",
        "        self.imgs = []\n",
        "        self.cats = []\n",
        "        if not annotation_file == None:\n",
        "            print('loading annotations into memory...')\n",
        "            time_t = datetime.datetime.utcnow()\n",
        "            dataset = json.load(open(annotation_file, 'r'))\n",
        "            print(datetime.datetime.utcnow() - time_t)\n",
        "            self.dataset = dataset\n",
        "            self.createIndex()\n",
        "\n",
        "    def createIndex(self):\n",
        "        # create index\n",
        "        print('creating index...')\n",
        "        imgToAnns = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
        "        anns =      {ann['id']:       [] for ann in self.dataset['annotations']}\n",
        "        for ann in self.dataset['annotations']:\n",
        "            imgToAnns[ann['image_id']] += [ann]\n",
        "            anns[ann['id']] = ann\n",
        "\n",
        "        imgs      = {im['id']: {} for im in self.dataset['images']}\n",
        "        for img in self.dataset['images']:\n",
        "            imgs[img['id']] = img\n",
        "\n",
        "        cats = []\n",
        "        catToImgs = []\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            cats = {cat['id']: [] for cat in self.dataset['categories']}\n",
        "            for cat in self.dataset['categories']:\n",
        "                cats[cat['id']] = cat\n",
        "            catToImgs = {cat['id']: [] for cat in self.dataset['categories']}\n",
        "            for ann in self.dataset['annotations']:\n",
        "                catToImgs[ann['category_id']] += [ann['image_id']]\n",
        "\n",
        "        print('index created!')\n",
        "\n",
        "        # create class members\n",
        "        self.anns = anns\n",
        "        self.imgToAnns = imgToAnns\n",
        "        self.catToImgs = catToImgs\n",
        "        self.imgs = imgs\n",
        "        self.cats = cats\n",
        "\n",
        "    def info(self):\n",
        "        \"\"\"\n",
        "        Print information about the annotation file.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for key, value in self.datset['info'].items():\n",
        "            print('%s: %s'%(key, value))\n",
        "\n",
        "    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n",
        "        \"\"\"\n",
        "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
        "        :param imgIds  (int array)     : get anns for given imgs\n",
        "               catIds  (int array)     : get anns for given cats\n",
        "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
        "               iscrowd (boolean)       : get anns for given crowd label (False or True)\n",
        "        :return: ids (int array)       : integer array of ann ids\n",
        "        \"\"\"\n",
        "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n",
        "            anns = self.dataset['annotations']\n",
        "        else:\n",
        "            if not len(imgIds) == 0:\n",
        "                anns = sum([self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns],[])\n",
        "            else:\n",
        "                anns = self.dataset['annotations']\n",
        "            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n",
        "            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            if not iscrowd == None:\n",
        "                ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n",
        "            else:\n",
        "                ids = [ann['id'] for ann in anns]\n",
        "        else:\n",
        "            ids = [ann['id'] for ann in anns]\n",
        "        return ids\n",
        "\n",
        "    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n",
        "        \"\"\"\n",
        "        filtering parameters. default skips that filter.\n",
        "        :param catNms (str array)  : get cats for given cat names\n",
        "        :param supNms (str array)  : get cats for given supercategory names\n",
        "        :param catIds (int array)  : get cats for given cat ids\n",
        "        :return: ids (int array)   : integer array of cat ids\n",
        "        \"\"\"\n",
        "        catNms = catNms if type(catNms) == list else [catNms]\n",
        "        supNms = supNms if type(supNms) == list else [supNms]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(catNms) == len(supNms) == len(catIds) == 0:\n",
        "            cats = self.dataset['categories']\n",
        "        else:\n",
        "            cats = self.dataset['categories']\n",
        "            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n",
        "            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n",
        "            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n",
        "        ids = [cat['id'] for cat in cats]\n",
        "        return ids\n",
        "\n",
        "    def getImgIds(self, imgIds=[], catIds=[]):\n",
        "        '''\n",
        "        Get img ids that satisfy given filter conditions.\n",
        "        :param imgIds (int array) : get imgs for given ids\n",
        "        :param catIds (int array) : get imgs with all given cats\n",
        "        :return: ids (int array)  : integer array of img ids\n",
        "        '''\n",
        "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
        "        catIds = catIds if type(catIds) == list else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            ids = self.imgs.keys()\n",
        "        else:\n",
        "            ids = set(imgIds)\n",
        "            for catId in catIds:\n",
        "                if len(ids) == 0:\n",
        "                    ids = set(self.catToImgs[catId])\n",
        "                else:\n",
        "                    ids &= set(self.catToImgs[catId])\n",
        "        return list(ids)\n",
        "\n",
        "    def loadAnns(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying anns\n",
        "        :return: anns (object array) : loaded ann objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.anns[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.anns[ids]]\n",
        "\n",
        "    def loadCats(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load cats with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying cats\n",
        "        :return: cats (object array) : loaded cat objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.cats[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.cats[ids]]\n",
        "\n",
        "    def loadImgs(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying img\n",
        "        :return: imgs (object array) : loaded img objects\n",
        "        \"\"\"\n",
        "        if type(ids) == list:\n",
        "            return [self.imgs[id_] for id_ in ids]\n",
        "        elif isinstance(ids, int):\n",
        "            return [self.imgs[ids]]\n",
        "\n",
        "    def showAnns(self, anns):\n",
        "        \"\"\"\n",
        "        Display the specified annotations.\n",
        "        :param anns (array of object): annotations to display\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        if len(anns) == 0:\n",
        "            return 0\n",
        "        if self.dataset['type'] == 'instances':\n",
        "            ax = plt.gca()\n",
        "            polygons = []\n",
        "            color = []\n",
        "            for ann in anns:\n",
        "                c = np.random.random((1, 3)).tolist()[0]\n",
        "                if type(ann['segmentation']) == list:\n",
        "                    # polygon\n",
        "                    for seg in ann['segmentation']:\n",
        "                        poly = np.array(seg).reshape((len(seg)//2, 2))\n",
        "                        polygons.append(Polygon(poly, True,alpha=0.4))\n",
        "                        color.append(c)\n",
        "                else:\n",
        "                    # mask\n",
        "                    mask = COCO.decodeMask(ann['segmentation'])\n",
        "                    img = np.ones( (mask.shape[0], mask.shape[1], 3) )\n",
        "                    if ann['iscrowd'] == 1:\n",
        "                        color_mask = np.array([2.0,166.0,101.0])/255\n",
        "                    if ann['iscrowd'] == 0:\n",
        "                        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "                    for i in range(3):\n",
        "                        img[:,:,i] = color_mask[i]\n",
        "                    ax.imshow(np.dstack( (img, mask*0.5) ))\n",
        "            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n",
        "            ax.add_collection(p)\n",
        "        if self.dataset['type'] == 'captions':\n",
        "            for ann in anns:\n",
        "                print(ann['caption'])\n",
        "\n",
        "    def loadRes(self, resFile):\n",
        "        \"\"\"\n",
        "        Load result file and return a result api object.\n",
        "        :param   resFile (str)     : file name of result file\n",
        "        :return: res (obj)         : result api object\n",
        "        \"\"\"\n",
        "        res = COCO()\n",
        "        res.dataset['images'] = [img for img in self.dataset['images']]\n",
        "        res.dataset['info'] = copy.deepcopy(self.dataset['info'])\n",
        "        res.dataset['type'] = copy.deepcopy(self.dataset['type'])\n",
        "        res.dataset['licenses'] = copy.deepcopy(self.dataset['licenses'])\n",
        "\n",
        "        print('Loading and preparing results...     ')\n",
        "        time_t = datetime.datetime.utcnow()\n",
        "        anns    = json.load(open(resFile))\n",
        "        assert type(anns) == list, 'results in not an array of objects'\n",
        "        annsImgIds = [ann['image_id'] for ann in anns]\n",
        "        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
        "               'Results do not correspond to current coco set'\n",
        "        if 'caption' in anns[0]:\n",
        "            imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n",
        "            res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n",
        "            for id_, ann in enumerate(anns):\n",
        "                ann['id'] = id_\n",
        "        elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n",
        "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "            for id_, ann in enumerate(anns):\n",
        "                bb = ann['bbox']\n",
        "                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n",
        "                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
        "                ann['area'] = bb[2]*bb[3]\n",
        "                ann['id'] = id_\n",
        "                ann['iscrowd'] = 0\n",
        "        elif 'segmentation' in anns[0]:\n",
        "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "            for id_, ann in enumerate(anns):\n",
        "                ann['area']=sum(ann['segmentation']['counts'][2:-1:2])\n",
        "                ann['bbox'] = []\n",
        "                ann['id'] = id_\n",
        "                ann['iscrowd'] = 0\n",
        "        print('DONE (t=%0.2fs)'%((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
        "\n",
        "        res.dataset['annotations'] = anns\n",
        "        res.createIndex()\n",
        "        return res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def decodeMask(R):\n",
        "        \"\"\"\n",
        "        Decode binary mask M encoded via run-length encoding.\n",
        "        :param   R (object RLE)    : run-length encoding of binary mask\n",
        "        :return: M (bool 2D array) : decoded binary mask\n",
        "        \"\"\"\n",
        "        N = len(R['counts'])\n",
        "        M = np.zeros( (R['size'][0]*R['size'][1], ))\n",
        "        n = 0\n",
        "        val = 1\n",
        "        for pos in range(N):\n",
        "            val = not val\n",
        "            for c in range(R['counts'][pos]):\n",
        "                R['counts'][pos]\n",
        "                M[n] = val\n",
        "                n += 1\n",
        "        return M.reshape((R['size']), order='F')\n",
        "\n",
        "    @staticmethod\n",
        "    def encodeMask(M):\n",
        "        \"\"\"\n",
        "        Encode binary mask M using run-length encoding.\n",
        "        :param   M (bool 2D array)  : binary mask to encode\n",
        "        :return: R (object RLE)     : run-length encoding of binary mask\n",
        "        \"\"\"\n",
        "        [h, w] = M.shape\n",
        "        M = M.flatten(order='F')\n",
        "        N = len(M)\n",
        "        counts_list = []\n",
        "        pos = 0\n",
        "        # counts\n",
        "        counts_list.append(1)\n",
        "        diffs = np.logical_xor(M[0:N-1], M[1:N])\n",
        "        for diff in diffs:\n",
        "            if diff:\n",
        "                pos +=1\n",
        "                counts_list.append(1)\n",
        "            else:\n",
        "                counts_list[pos] += 1\n",
        "        # if array starts from 1. start with 0 counts for 0\n",
        "        if M[0] == 1:\n",
        "            counts_list = [0] + counts_list\n",
        "        return {'size':      [h, w],\n",
        "               'counts':    counts_list ,\n",
        "               }\n",
        "\n",
        "    @staticmethod\n",
        "    def segToMask( S, h, w ):\n",
        "         \"\"\"\n",
        "         Convert polygon segmentation to binary mask.\n",
        "         :param   S (float array)   : polygon segmentation mask\n",
        "         :param   h (int)           : target mask height\n",
        "         :param   w (int)           : target mask width\n",
        "         :return: M (bool 2D array) : binary mask\n",
        "         \"\"\"\n",
        "         M = np.zeros((h,w), dtype=np.bool)\n",
        "         for s in S:\n",
        "             N = len(s)\n",
        "             rr, cc = polygon(np.array(s[1:N:2]), np.array(s[0:N:2])) # (y, x)\n",
        "             M[rr, cc] = 1\n",
        "         return M"
      ],
      "metadata": {
        "id": "WC8YSfVBDinF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval.py"
      ],
      "metadata": {
        "id": "KzL7PepKDcfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'tylin'\n",
        "from builtins import dict\n",
        "# from .tokenizer.ptbtokenizer import PTBTokenizer\n",
        "# from .bleu.bleu import Bleu\n",
        "# from .meteor.meteor import Meteor\n",
        "# from .rouge.rouge import Rouge\n",
        "# from .cider.cider import Cider\n",
        "# from .spice.spice import Spice\n",
        "\n",
        "class COCOEvalCap:\n",
        "    def __init__(self, coco, cocoRes):\n",
        "        self.evalImgs = []\n",
        "        self.eval = dict()\n",
        "        self.imgToEval = dict()\n",
        "        self.coco = coco\n",
        "        self.cocoRes = cocoRes\n",
        "        self.params = {'image_id': coco.getImgIds()}\n",
        "\n",
        "        self.gts = None\n",
        "        self.res = None\n",
        "        # self.df = df\n",
        "\n",
        "    def tokenize(self):\n",
        "        imgIds = self.params['image_id']\n",
        "        # imgIds = self.coco.getImgIds()\n",
        "        gts = dict()\n",
        "        res = dict()\n",
        "        for imgId in imgIds:\n",
        "            gts[imgId] = self.coco.imgToAnns[imgId]\n",
        "            res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenization...')\n",
        "        tokenizer = PTBTokenizer()\n",
        "        self.gts  = tokenizer.tokenize(gts)\n",
        "        self.res = tokenizer.tokenize(res)\n",
        "        # print(len(self.gts))\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.tokenize()\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"])\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(self.gts, self.res)\n",
        "            print(score)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, self.gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, self.gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = dict()\n",
        "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
      ],
      "metadata": {
        "id": "USQS0bQHDAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9999VzDDRW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EvalCapDemo"
      ],
      "metadata": {
        "id": "IjFfVOd-D82E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "from pycocotools.coco import COCO\n",
        "# from pycocoevalcap.eval import COCOEvalCap\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "import json\n",
        "from json import encoder\n",
        "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
      ],
      "metadata": {
        "id": "DlKX3I9RDRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create coco object and cocoRes object\n",
        "coco = COCO('/content/drive/MyDrive/논문/captions_val2014.json')\n",
        "cocoRes = coco.loadRes('/content/drive/MyDrive/논문/cocores.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1rHSD_UEALn",
        "outputId": "6a2ee7e4-c6e9-430b-f39d-a3c8343965d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.99s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create cocoEval object by taking coco and cocoRes\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "\n",
        "# evaluate on a subset of images by setting\n",
        "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "# please remove this line when evaluating the full validation set\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "\n",
        "# evaluate results\n",
        "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
        "cocoEval.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsUyo66JEKWh",
        "outputId": "a134b5d4-a3c6-4f83-ae32-35b36f073f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'testlen': 394647, 'reflen': 389817, 'guess': [394647, 354457, 314267, 274077], 'correct': [309680, 181692, 95661, 49839]}\n",
            "ratio: 1.0123904293553103\n",
            "[0.7847012646745046, 0.6342176152680618, 0.4965590625075106, 0.3862801276888551]\n",
            "Bleu_1: 0.785\n",
            "Bleu_2: 0.634\n",
            "Bleu_3: 0.497\n",
            "Bleu_4: 0.386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBJLcekrEsyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}